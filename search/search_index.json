{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"BPDecoderPlus","text":"<p>Quantum Error Correction with Belief Propagation</p> <p>Work in Progress</p> <p>This WIP project is for AI + Quantum winter school training.</p> <p>A winter school project on circuit-level decoding of surface codes using belief propagation and integer programming decoders, with extensions for atom loss in neutral atom quantum computers.</p>"},{"location":"#project-goals","title":"Project Goals","text":"Level Task Description Basic MLE Decoder Reproduce the integer programming (MLE) decoder as the baseline Challenge Atom Loss Handle atom loss errors in neutral atom systems Extension QEC Visualization https://github.com/nzy1997/qec-thrust <p>Info</p> <p>We also want to explore the boundary of vibe coding, which may lead to a scipost paper.</p>"},{"location":"#learning-objectives","title":"Learning Objectives","text":"<p>After completing this project, students will:</p> <ul> <li>Understand surface code structure and syndrome extraction</li> <li>Implement and compare different decoding algorithms</li> <li>Analyze decoder performance through threshold plots</li> <li>Learn about practical QEC challenges (atom loss, circuit-level noise)</li> </ul>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Programming: Julia basics, familiarity with Python for plotting</li> <li>Mathematics: Linear algebra, probability theory</li> <li>QEC Background: Stabilizer formalism, surface codes (helpful but not required)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install the package:</p> <pre><code># Clone the repository\ngit clone https://github.com/GiggleLiu/BPDecoderPlus.git\ncd BPDecoderPlus\n\n# Install dependencies\nmake setup\n\n# Run tests\nmake test\n</code></pre> <p>Generate a dataset:</p> <pre><code>python -m bpdecoderplus.cli \\\n  --distance 3 \\\n  --p 0.01 \\\n  --rounds 3 5 7 \\\n  --generate-dem \\\n  --generate-syndromes 1000\n</code></pre>"},{"location":"#features","title":"Features","text":""},{"location":"#python-module-bpdecoderplus","title":"Python Module (bpdecoderplus)","text":"<ul> <li>Noisy Circuit Generation: Create surface code circuits with realistic noise models</li> <li>Detector Error Model (DEM): Extract error models for belief propagation</li> <li>UAI Format Support: Export to UAI format for probabilistic inference with TensorInference.jl</li> <li>Syndrome Database: Generate training datasets from circuit simulations</li> <li>PyTorch BP Implementation: Belief propagation solver for factor graphs</li> </ul>"},{"location":"#julia-module-tensorqecjl-integration","title":"Julia Module (TensorQEC.jl Integration)","text":"<ul> <li>Multiple decoder implementations (IP, BP, BP+OSD, Matching)</li> <li>Comprehensive benchmarking tools</li> <li>Performance visualization</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started - Quick start guide and pipeline overview</li> <li>Usage Guide - Detailed usage examples</li> <li>API Reference - Complete API documentation</li> <li>Mathematical Description - Mathematical background</li> </ul>"},{"location":"#available-decoders","title":"Available Decoders","text":"Decoder Symbol Description IP (MLE) <code>:IP</code> Integer programming decoder - finds minimum weight error BP <code>:BP</code> Belief propagation without post-processing BP+OSD <code>:BPOSD</code> BP with Ordered Statistics Decoding post-processing Matching <code>:Matching</code> Minimum weight perfect matching (via TensorQEC)"},{"location":"#resources","title":"Resources","text":""},{"location":"#core-library","title":"Core Library","text":"<ul> <li>TensorQEC.jl - QEC library we build on</li> </ul>"},{"location":"#reference-implementations","title":"Reference Implementations","text":"<ul> <li>bp_osd - Python BP+OSD implementation</li> <li>ldpc - LDPC decoder library</li> </ul>"},{"location":"#documentation_1","title":"Documentation","text":"<ul> <li>TensorQEC Documentation</li> <li>Error Correction Zoo</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License - See LICENSE file for details.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This project is built on TensorQEC.jl by nzy1997.</p>"},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/#pytorch-bp-api-reference","title":"PyTorch BP API Reference","text":"<p>This reference documents the public API exported from <code>bpdecoderplus.pytorch_bp</code>.</p>"},{"location":"api_reference/#uai-parsing","title":"UAI Parsing","text":"<ul> <li> <p><code>read_model_file(path, factor_eltype=torch.float64) -&gt; UAIModel</code>   Parse a UAI <code>.uai</code> model file.</p> </li> <li> <p><code>read_model_from_string(content, factor_eltype=torch.float64) -&gt; UAIModel</code>   Parse a UAI model from an in-memory string.</p> </li> <li> <p><code>read_evidence_file(path) -&gt; Dict[int, int]</code>   Parse a UAI <code>.evid</code> file and return evidence as 1-based indices.</p> </li> </ul>"},{"location":"api_reference/#data-structures","title":"Data Structures","text":"<ul> <li> <p><code>Factor(vars: List[int], values: torch.Tensor)</code>   Container for a factor scope and its tensor.</p> </li> <li> <p><code>UAIModel(nvars: int, cards: List[int], factors: List[Factor])</code>   Holds all model metadata for BP.</p> </li> </ul>"},{"location":"api_reference/#belief-propagation","title":"Belief Propagation","text":"<ul> <li> <p><code>BeliefPropagation(uai_model: UAIModel)</code>   Builds factor graph adjacency for BP.</p> </li> <li> <p><code>initial_state(bp: BeliefPropagation) -&gt; BPState</code>   Initialize messages to uniform vectors.</p> </li> <li> <p><code>collect_message(bp, state, normalize=True)</code>   Update factor-to-variable messages in place.</p> </li> <li> <p><code>process_message(bp, state, normalize=True, damping=0.2)</code>   Update variable-to-factor messages in place.</p> </li> <li> <p><code>belief_propagate(bp, max_iter=100, tol=1e-6, damping=0.2, normalize=True)</code>   Run the full BP loop and return <code>(BPState, BPInfo)</code>.</p> </li> <li> <p><code>compute_marginals(state, bp) -&gt; Dict[int, torch.Tensor]</code>   Compute marginal distributions after convergence.</p> </li> <li> <p><code>apply_evidence(bp, evidence: Dict[int, int]) -&gt; BeliefPropagation</code>   Return a new BP object with evidence applied to factor tensors.</p> </li> </ul>"},{"location":"getting_started/","title":"Getting Started with BPDecoderPlus","text":""},{"location":"getting_started/#overview","title":"Overview","text":"<p>BPDecoderPlus generates training and test data for belief propagation (BP) decoding of quantum error correction codes. This guide shows you how to generate circuits, extract error models, and sample syndrome data - everything you need to train and test quantum error correction decoders.</p>"},{"location":"getting_started/#quick-start","title":"Quick Start","text":"<p>Generate a complete dataset with one command:</p> <pre><code>python -m bpdecoderplus.cli \\\n  --distance 3 \\\n  --p 0.01 \\\n  --rounds 3 \\\n  --task z \\\n  --generate-dem \\\n  --generate-uai \\\n  --generate-syndromes 1000\n</code></pre> <p>This creates files in organized subdirectories: <pre><code>datasets/\n\u251c\u2500\u2500 circuits/sc_d3_r3_p0010_z.stim    # Noisy quantum circuit\n\u251c\u2500\u2500 dems/sc_d3_r3_p0010_z.dem         # Detector error model\n\u251c\u2500\u2500 uais/sc_d3_r3_p0010_z.uai         # UAI format for inference\n\u2514\u2500\u2500 syndromes/sc_d3_r3_p0010_z.npz    # 1000 syndrome samples\n</code></pre></p>"},{"location":"getting_started/#understanding-the-pipeline","title":"Understanding the Pipeline","text":"<p>Data generation happens in four steps:</p> <ol> <li>Generate Noisy Circuit - Creates a surface code circuit with realistic noise</li> <li>Extract Detector Error Model (DEM) - Analyzes which errors trigger which detectors</li> <li>Convert to Inference Formats - Exports DEM as .dem and .uai files</li> <li>Sample Syndromes - Runs the circuit many times to generate training data</li> </ol>"},{"location":"getting_started/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"getting_started/#step-1-generate-noisy-circuit","title":"Step 1: Generate Noisy Circuit","text":"<p>Create a quantum error correction circuit with realistic noise.</p> <p>Parameters: - <code>--distance</code> - Size of the surface code (3, 5, 7, etc.) - <code>--rounds</code> - Number of error correction cycles - <code>--p</code> - Physical error rate (e.g., 0.01 = 1% error per operation) - <code>--task</code> - Type of logical operation (\"z\" for memory experiment)</p> <p>Example: <pre><code>python -m bpdecoderplus.cli --distance 3 --p 0.01 --rounds 3 --task z\n</code></pre></p> <p>Output: <code>datasets/circuits/sc_d3_r3_p0010_z.stim</code></p> <p>The <code>.stim</code> file contains the complete quantum circuit with: - Qubit initialization - Syndrome measurement operations - Noise on every gate - Logical observable measurement</p>"},{"location":"getting_started/#step-2-extract-detector-error-model-dem","title":"Step 2: Extract Detector Error Model (DEM)","text":"<p>The DEM tells us which errors trigger which syndrome detectors.</p> <p>Why we need it: The circuit describes quantum operations, but the decoder needs to know: - What errors can occur? Decoder use it to decides what types of error it can output. - Which detectors fire when each error happens? This eventually transformed into a decoder graph/tanner graph/Factor graph. - Does the error flip the logical qubit? This used as a benchmark for logical error rate.</p> <p>Generate DEM: <pre><code>python -m bpdecoderplus.cli --distance 3 --p 0.01 --rounds 3 --task z --generate-dem\n</code></pre></p> <p>Output: <code>datasets/dems/sc_d3_r3_p0010_z.dem</code></p> <p>The <code>.dem</code> file contains entries like: <pre><code>error(0.01) D0 D5 L0\n</code></pre> This means: \"There's a 1% chance of an error that triggers detectors 0 and 5, and flips the logical observable\"</p> <p>Using in Python: <pre><code>from bpdecoderplus.dem import extract_dem, build_parity_check_matrix\nimport stim\n\n# Load circuit and extract DEM\ncircuit = stim.Circuit.from_file('datasets/circuits/sc_d3_r3_p0010_z.stim')\ndem = extract_dem(circuit)\n\n# Build parity check matrix for BP decoding\nH, priors, obs_flip = build_parity_check_matrix(dem)\nprint(f\"H matrix shape: {H.shape}\")  # (24, 286) for d=3, r=3\n</code></pre></p> <p>The parity check matrix H is the mathematical representation needed for BP decoding: - <code>H[i,j] = 1</code> if error j triggers detector i - <code>priors[j]</code> = probability of error j occurring - <code>obs_flip[j] = 1</code> if error j flips the logical observable</p>"},{"location":"getting_started/#step-3-generate-uai-format-optional","title":"Step 3: Generate UAI Format (Optional)","text":"<p>The UAI format enables probabilistic inference with tools like TensorInference.jl.</p> <p>What is UAI format? UAI (Uncertainty in Artificial Intelligence) is a standard format for representing probabilistic graphical models. It represents the DEM as a Markov network where: - Each detector is a binary variable (0 or 1) - Each error mechanism is a factor/clique - Factor tables encode error probabilities</p> <p>Generate UAI: <pre><code>python -m bpdecoderplus.cli --distance 3 --p 0.01 --rounds 3 --task z --generate-uai\n</code></pre></p> <p>Output: <code>datasets/uais/sc_d3_r3_p0010_z.uai</code></p> <p>The <code>.uai</code> file structure: <pre><code>MARKOV\n24                    # Number of variables (detectors)\n2 2 2 2 ...          # Each variable has 2 states (0 or 1)\n286                   # Number of factors (error mechanisms)\n1 0                   # Factor 1 involves detector 0\n2 0 1                 # Factor 2 involves detectors 0 and 1\n...\n</code></pre></p> <p>Using UAI files: UAI files can be used with probabilistic inference tools for: - Exact inference (partition function calculation) - Marginal probability computation - MAP (maximum a posteriori) inference - Integration with TensorInference.jl for tensor network methods</p>"},{"location":"getting_started/#step-4-sample-syndromes","title":"Step 4: Sample Syndromes","text":"<p>Generate training/test data by running the circuit many times.</p> <p>What happens: Each \"shot\" runs the full circuit: 1. Initialize qubits 2. Apply gates (errors occur randomly based on noise rate) 3. Measure syndromes (which detectors fire?) 4. Measure logical qubit (did it flip?)</p> <p>Generate syndromes: <pre><code>python -m bpdecoderplus.cli \\\n  --distance 3 --p 0.01 --rounds 3 --task z \\\n  --generate-syndromes 10000\n</code></pre></p> <p>Output: <code>datasets/syndromes/sc_d3_r3_p0010_z.npz</code></p> <p>The <code>.npz</code> file contains: - <code>syndromes</code>: Binary array (num_shots \u00d7 num_detectors) - <code>observables</code>: Binary array (num_shots,) - 1 means logical error - <code>metadata</code>: Circuit parameters (JSON)</p> <p>Loading syndrome data: <pre><code>from bpdecoderplus.syndrome import load_syndrome_database\n\nsyndromes, observables, metadata = load_syndrome_database(\n    'datasets/syndromes/sc_d3_r3_p0010_z.npz'\n)\n\nprint(f\"Syndromes shape: {syndromes.shape}\")      # (10000, 24)\nprint(f\"Observables shape: {observables.shape}\")  # (10000,)\nprint(f\"Metadata: {metadata}\")\n</code></pre></p>"},{"location":"getting_started/#understanding-the-data","title":"Understanding the Data","text":""},{"location":"getting_started/#syndromes-detection-events","title":"Syndromes (Detection Events)","text":"<p>Each row is a syndrome - a binary vector indicating which detectors fired:</p> <pre><code>syndrome = syndromes[0]  # First shot\n# Example: [0, 1, 1, 0, 0, 0, 1, 0, ...]\n#           \u2191  \u2191  \u2191           \u2191\n#           Detectors 1, 2, and 6 fired\n</code></pre> <p>What does a detection event mean? - A detector fires (value = 1) when there's a change in the syndrome between consecutive measurement rounds - This indicates an error occurred in that space-time region - The decoder's job is to infer which errors caused these detection events</p>"},{"location":"getting_started/#observables-logical-outcomes","title":"Observables (Logical Outcomes)","text":"<p>Each observable value indicates whether the logical qubit flipped:</p> <pre><code>observable = observables[0]  # First shot\n# 0 = No logical error (decoder should predict 0)\n# 1 = Logical error occurred (decoder should predict 1)\n</code></pre> <p>Decoder success criterion: - Decoder predicts observable flip from syndrome - If prediction matches actual observable \u2192 Success - If prediction differs \u2192 Logical error</p>"},{"location":"getting_started/#complete-example-workflow","title":"Complete Example Workflow","text":""},{"location":"getting_started/#generate-all-data-at-once","title":"Generate all data at once","text":"<pre><code>python -m bpdecoderplus.cli \\\n  --distance 3 \\\n  --p 0.01 \\\n  --rounds 3 5 7 \\\n  --task z \\\n  --generate-dem \\\n  --generate-uai \\\n  --generate-syndromes 10000\n</code></pre> <p>This creates a complete dataset for three different round counts: <pre><code>datasets/\n\u251c\u2500\u2500 circuits/\n\u2502   \u251c\u2500\u2500 sc_d3_r3_p0010_z.stim\n\u2502   \u251c\u2500\u2500 sc_d3_r5_p0010_z.stim\n\u2502   \u2514\u2500\u2500 sc_d3_r7_p0010_z.stim\n\u251c\u2500\u2500 dems/\n\u2502   \u251c\u2500\u2500 sc_d3_r3_p0010_z.dem\n\u2502   \u251c\u2500\u2500 sc_d3_r5_p0010_z.dem\n\u2502   \u2514\u2500\u2500 sc_d3_r7_p0010_z.dem\n\u251c\u2500\u2500 uais/\n\u2502   \u251c\u2500\u2500 sc_d3_r3_p0010_z.uai\n\u2502   \u251c\u2500\u2500 sc_d3_r5_p0010_z.uai\n\u2502   \u2514\u2500\u2500 sc_d3_r7_p0010_z.uai\n\u2514\u2500\u2500 syndromes/\n    \u251c\u2500\u2500 sc_d3_r3_p0010_z.npz\n    \u251c\u2500\u2500 sc_d3_r5_p0010_z.npz\n    \u2514\u2500\u2500 sc_d3_r7_p0010_z.npz\n</code></pre></p>"},{"location":"getting_started/#use-in-python","title":"Use in Python","text":"<pre><code>import numpy as np\nfrom bpdecoderplus.dem import extract_dem, build_parity_check_matrix\nfrom bpdecoderplus.syndrome import load_syndrome_database\nimport stim\n\n# Load circuit and extract DEM\ncircuit = stim.Circuit.from_file('datasets/circuits/sc_d3_r3_p0010_z.stim')\ndem = extract_dem(circuit)\n\n# Build parity check matrix\nH, priors, obs_flip = build_parity_check_matrix(dem)\nprint(f\"H matrix shape: {H.shape}\")  # (24, 286) for d=3, r=3\n\n# Load syndrome data\nsyndromes, observables, metadata = load_syndrome_database(\n    'datasets/syndromes/sc_d3_r3_p0010_z.npz'\n)\n\n# Ready for BP decoder!\n# decoder = BPDecoder(H, priors, obs_flip)  # Coming soon\n# predictions = decoder.decode(syndromes)\n# accuracy = (predictions == observables).mean()\n</code></pre>"},{"location":"getting_started/#use-cases","title":"Use Cases","text":""},{"location":"getting_started/#1-decoder-training","title":"1. Decoder Training","text":"<pre><code># Load training data\nsyndromes, observables, _ = load_syndrome_database(\"train.npz\")\n\n# Train decoder\ndecoder.fit(syndromes, observables)\n</code></pre>"},{"location":"getting_started/#2-decoder-evaluation","title":"2. Decoder Evaluation","text":"<pre><code># Load test data\nsyndromes, actual_obs, _ = load_syndrome_database(\"test.npz\")\n\n# Predict\npredicted_obs = decoder.predict(syndromes)\n\n# Evaluate\naccuracy = (predicted_obs == actual_obs).mean()\nlogical_error_rate = 1 - accuracy\nprint(f\"Logical error rate: {logical_error_rate:.4f}\")\n</code></pre>"},{"location":"getting_started/#3-decoder-comparison","title":"3. Decoder Comparison","text":"<pre><code># Compare BP vs MWPM vs Neural decoder\nfor decoder in [bp_decoder, mwpm_decoder, neural_decoder]:\n    predictions = decoder.predict(syndromes)\n    error_rate = (predictions != observables).mean()\n    print(f\"{decoder.name}: {error_rate:.4f}\")\n</code></pre>"},{"location":"getting_started/#advanced-usage","title":"Advanced Usage","text":""},{"location":"getting_started/#generate-multiple-noise-levels","title":"Generate multiple noise levels","text":"<p>For comprehensive testing, generate data at different noise levels:</p> <pre><code># Low noise\npython -m bpdecoderplus.cli --distance 3 --p 0.001 --rounds 3 --task z \\\n  --generate-dem --generate-uai --generate-syndromes 10000\n\n# Medium noise\npython -m bpdecoderplus.cli --distance 3 --p 0.01 --rounds 3 --task z \\\n  --generate-dem --generate-uai --generate-syndromes 10000\n\n# High noise\npython -m bpdecoderplus.cli --distance 3 --p 0.05 --rounds 3 --task z \\\n  --generate-dem --generate-uai --generate-syndromes 10000\n</code></pre>"},{"location":"getting_started/#different-code-distances","title":"Different code distances","text":"<pre><code># Small code (fast, lower threshold)\npython -m bpdecoderplus.cli --distance 3 --p 0.01 --rounds 3 --task z \\\n  --generate-dem --generate-syndromes 10000\n\n# Larger code (slower, higher threshold)\npython -m bpdecoderplus.cli --distance 5 --p 0.01 --rounds 5 --task z \\\n  --generate-dem --generate-syndromes 10000\n</code></pre>"},{"location":"getting_started/#custom-sampling","title":"Custom sampling","text":"<pre><code>from bpdecoderplus.syndrome import sample_syndromes, save_syndrome_database\nimport stim\n\n# Load circuit\ncircuit = stim.Circuit.from_file(\"datasets/circuits/sc_d3_r3_p0010_z.stim\")\n\n# Sample with custom shots\nsyndromes, observables = sample_syndromes(circuit, num_shots=100000)\n\n# Save with metadata\nmetadata = {\"description\": \"Large training set\", \"purpose\": \"neural decoder\"}\nsave_syndrome_database(\n    syndromes, observables,\n    \"datasets/syndromes/large_train.npz\",\n    metadata\n)\n</code></pre>"},{"location":"getting_started/#expected-performance","title":"Expected Performance","text":"<p>For a distance-3 surface code with p=0.01:</p> Property Expected Value Number of detectors 24 (8 per round \u00d7 3 rounds) Number of error mechanisms ~286 Detection event rate 3-5% per detector Observable flip rate ~0.5-1% Non-trivial syndromes &gt;90% <p>The BP decoder should reduce the logical error rate by 3-10x compared to no decoding.</p>"},{"location":"getting_started/#file-format-reference","title":"File Format Reference","text":""},{"location":"getting_started/#circuit-file-stim","title":"Circuit File (.stim)","text":"<p>Contains quantum operations and noise model. Handled automatically by the package.</p>"},{"location":"getting_started/#dem-file-dem","title":"DEM File (.dem)","text":"<p>Lists all error mechanisms and their effects: <pre><code>error(0.01) D0 D1      # Error triggers detectors 0 and 1\nerror(0.01) D1 D2      # Error triggers detectors 1 and 2\nerror(0.01) D0 D2 L0   # Error triggers detectors 0, 2 and flips logical\n</code></pre></p>"},{"location":"getting_started/#uai-file-uai","title":"UAI File (.uai)","text":"<p>Markov network representation for probabilistic inference: <pre><code>MARKOV                 # Network type\n24                     # Number of variables\n2 2 2 ...             # Variable cardinalities\n286                    # Number of factors\n1 0                    # Factor scopes\n2 0 1\n...\n</code></pre></p>"},{"location":"getting_started/#syndrome-database-npz","title":"Syndrome Database (.npz)","text":"<p>NumPy archive with three components: <pre><code>data = np.load('sc_d3_r3_p0010_z.npz')\nsyndromes = data['syndromes']      # Shape: (num_shots, num_detectors)\nobservables = data['observables']  # Shape: (num_shots,)\nmetadata = data['metadata']        # JSON string with parameters\n</code></pre></p>"},{"location":"getting_started/#troubleshooting","title":"Troubleshooting","text":"<p>Q: The command is slow A: Syndrome sampling is the slowest part. Start with fewer shots (e.g., 1000) for testing.</p> <p>Q: Files are large A: The .npz files scale with num_shots. Use 1000-10000 shots for development, more for final evaluation.</p> <p>Q: What's a good starting point? A: Use distance=3, rounds=3, p=0.01, and 10000 shots. This runs quickly and gives good statistics.</p> <p>Q: When should I use UAI format? A: Use UAI format if you want to: - Integrate with TensorInference.jl or other probabilistic inference tools - Perform exact inference or marginal probability calculations - Use tensor network methods for decoding</p>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<ol> <li>Generate your first dataset using the Quick Start command</li> <li>Explore the data by loading the .npz file and examining syndromes</li> <li>Experiment with different parameters (distance, rounds, noise rate)</li> <li>Wait for BP decoder implementation to evaluate decoder performance</li> </ol>"},{"location":"getting_started/#references","title":"References","text":"<ul> <li>Stim Documentation - Circuit simulation and sampling</li> <li>TensorInference.jl - UAI format and tensor network inference</li> <li>Surface Code Decoding - Decoder review</li> <li>BP+OSD Paper - BP decoder with OSD post-processing</li> </ul>"},{"location":"getting_started/#support","title":"Support","text":"<p>For issues or questions: - Check test suite: <code>tests/test_*.py</code> - See minimal example: <code>examples/minimal_example.py</code> - Report issues: GitHub Issues</p>"},{"location":"mathematical_description/","title":"Mathematical Description","text":""},{"location":"mathematical_description/#belief-propagation-bp-overview","title":"Belief Propagation (BP) Overview","text":"<p>This document summarizes the BP message-passing rules implemented in <code>src/bpdecoderplus/pytorch_bp/belief_propagation.py</code> for discrete factor graphs. The approach mirrors the tensor-contraction perspective used in TensorInference.jl. See https://github.com/TensorBFS/TensorInference.jl for the Julia reference.</p>"},{"location":"mathematical_description/#factor-graph-notation","title":"Factor Graph Notation","text":"<ul> <li>Variables are indexed by x_i with domain size d_i.</li> <li>Factors are indexed by f and connect a subset of variables.</li> <li>Each factor has a tensor (potential) phi_f defined over its variables.</li> </ul>"},{"location":"mathematical_description/#messages","title":"Messages","text":"<p>Factor to variable message:</p> <p>mu_{f-&gt;x}(x) = sum_{all y in ne(f), y != x} phi_f(x, y, ...) * product_{y != x} mu_{y-&gt;f}(y)</p> <p>Variable to factor message:</p> <p>mu_{x-&gt;f}(x) = product_{g in ne(x), g != f} mu_{g-&gt;x}(x)</p>"},{"location":"mathematical_description/#damping","title":"Damping","text":"<p>To improve stability on loopy graphs, a damping update is applied:</p> <p>mu_new = damping * mu_old + (1 - damping) * mu_candidate</p>"},{"location":"mathematical_description/#convergence","title":"Convergence","text":"<p>We use an L1 difference threshold between consecutive factor-&gt;variable messages to determine convergence.</p>"},{"location":"mathematical_description/#marginals","title":"Marginals","text":"<p>After convergence, variable marginals are computed as:</p> <p>b(x) = (1 / Z) * product_{f in ne(x)} mu_{f-&gt;x}(x)</p> <p>The normalization constant Z is obtained by summing the unnormalized vector.</p>"},{"location":"usage_guide/","title":"Usage Guide","text":""},{"location":"usage_guide/#pytorch-belief-propagation-usage","title":"PyTorch Belief Propagation Usage","text":"<p>This guide shows how to parse a UAI file, run BP, and apply evidence. The implementation follows the tensor-contraction viewpoint in TensorInference.jl: https://github.com/TensorBFS/TensorInference.jl</p>"},{"location":"usage_guide/#quick-start","title":"Quick Start","text":"<pre><code>from bpdecoderplus.pytorch_bp import (\n    read_model_file,\n    BeliefPropagation,\n    belief_propagate,\n    compute_marginals,\n)\n\nmodel = read_model_file(\"examples/simple_model.uai\")\nbp = BeliefPropagation(model)\nstate, info = belief_propagate(bp, max_iter=50, tol=1e-8, damping=0.1)\nprint(info)\n\nmarginals = compute_marginals(state, bp)\nprint(marginals[1])\n</code></pre>"},{"location":"usage_guide/#evidence","title":"Evidence","text":"<pre><code>from bpdecoderplus.pytorch_bp import read_model_file, read_evidence_file, apply_evidence\nfrom bpdecoderplus.pytorch_bp import BeliefPropagation, belief_propagate, compute_marginals\n\nmodel = read_model_file(\"examples/simple_model.uai\")\nevidence = read_evidence_file(\"examples/simple_model.evid\")\nbp = apply_evidence(BeliefPropagation(model), evidence)\nstate, info = belief_propagate(bp)\nmarginals = compute_marginals(state, bp)\n</code></pre>"},{"location":"usage_guide/#tips","title":"Tips","text":"<ul> <li>For loopy graphs, use damping between 0.1 and 0.5.</li> <li>Normalize messages to avoid numerical underflow.</li> <li>Use float64 for consistent comparisons in tests.</li> </ul>"}]}